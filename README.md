# üßô Great Expectations Lambda
A serverless and event-driven approach to build data quality pipeline with AWS Lambda and Great Expectations

<br />

## üìú Table of Contents
- [About](#about)
- [Getting Started](#getting_started)
- [Usage](#usage)
- [Deploy](#deploy)
- [Built Using](#built_using)
- [Authors](#authors)

<br />

## üßê About <a name = "about"></a>
**Great Expectations** is an open-source data quality framework based on Python. GE enables engineers to write tests, review reports, and assess the quality of data. 
It is a plugable tool, meaning you can easily add new expectations and customize final reports.

**AWS Lambda** is a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers.

Unfortunately, AWS Lambda imposes certain quotas and limits on the size of the deployment package:
- **50 MB** (zipped, for direct upload) 
- **250 MB** (unzipped). This quota applies to all the files you upload, including layers and custom runtimes.

As a result, deploying GE on lambda takes some ingenuity. However, we can solve this problem by packaging and deploying Lambda functions as container images of up to **10 GB**

<br />

## üèÅ Getting Started <a name = "getting_started"></a>

Install packages in the virtualenv:

```
pipenv install --dev
```
<br />

## üíª Usage <a name="usage"></a>
Make sure to have Docker installed
```
docker --version
```

<br />

Run the following script to build the docker image, run the container and locally test the lambda function (AWS account not needed)
```
script/docker.sh
```

<br />

Run the following script to locally export the HTML documentation generated by Great Expectations. If no local path is specified ```C:/great_expectations_data_docs/``` will be used as default

```
script/export_data_docs.sh example/local/path
```

<br />

Go to the exported folder and open the ```index.html``` file


![img/ge_data_docs.png](img/ge_data_docs.png)

<br />

## üöÄ Deploy <a name = "deploy"></a>
I personally recommend [Serverless](https://www.serverless.com/) to deploy lambda functions.
Alternatively, in the ```infra``` folder you can find a Terraform example to create the AWS infrastructure. 

I've also added a script example to tag and push the docker image to **Amazon ECR** and automatically update the lambda code with the newly pushed images.

```
script/naive_deploy.sh
```

<br />

## ‚õèÔ∏è Built Using <a name = "built_using"></a>
- [Python](https://www.python.org/) | Programming language
- [Pipenv](https://pipenv.pypa.io/en/latest/) | Dependency management
- [Pre-Commit](https://pre-commit.com/) | Managing and maintaining hooks
- [Github Actions](https://github.com/features/actions) | CI/CD
- [Terraform](https://www.terraform.io/) | Infrastructure as Code
- [Docker](https://www.docker.com/) | Containerization and Deploy
- [Pandas](https://pandas.pydata.org/) | Data analysis and manipulation
- [python-lambda-local](https://github.com/HDE/python-lambda-local) | Run AWS lambda functions on local machine
- [great_expectations](https://greatexpectations.io/) | Data Quality framework
- [AWS lambda function](https://aws.amazon.com/lambda/) | Serverless compute service
- [Unix shell]() | Command-line interpreter

<br />

## ‚úèÔ∏è Authors <a name = "authors"></a>
- Made with ‚ù§Ô∏è  by [@vittoriopolverino](https://github.com/vittoriopolverino)
